<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mark  Crowley | Automated Materials Design and Discovery Using Reinforcement Learning</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/chemgymrl/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Mark</span>   Crowley
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/bio/">
                bio
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/news/">
                news
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
      
          
            <h1 class="post-title">Automated Materials Design and Discovery Using Reinforcement Learning</h1>
        
      
    <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:10px;">Studying how to automate material synthesis and discovery by training a Deep Reinforcement Learning system to plan and carry out chemical synthesis experiments to gather data and find efficient pathways to making new or known materials.</p>

      
        
        <p class="post-description" style="border-bottom-style:solid; border-bottom-color:lightgrey; border-bottom-width:3px; margin:1px;">
        <b>DOMAINS</b>
        
            
            | <a href="/ai-for-material-design/">ai-for-material-design</a> 
        
            
            | <a href="/digital-chemistry/">digital-chemistry</a> 
        
            
            | <a href="/ai-for-physics/">ai-for-physics</a> 
        
            
            | <a href="/ai-for-science/">ai-for-science</a> 
        
        </p>
        
      
      
      
      
        
        <p class="post-description" style="border-bottom-style:solid; border-bottom-color:lightgrey; border-bottom-width:3px; margin:1px;">
        <b>METHODS</b>
        
            
            | <a href="/reinforcement-learning/">reinforcement-learning</a> 
        
            
            | <a href="/LSTM/">LSTM</a> 
        
            
            | <a href="/LRCN/">LRCN</a> 
        
            
            | <a href="/MARL/">MARL</a> 
        
            
            | <a href="/MDP/">MDP</a> 
        
        </p>
        
      
        
            <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:3px;">
            <b>WEBPAGE: </b> 
        
            | <a href="https://uwaterloo.ca/scholar/mcrowley/dblstudy">https://uwaterloo.ca/scholar/mcrowley/dblstudy</a> 
         
            </p>
        
        <p></p>
  </header>

  <article>
      
      <img src="/assets/img/chemgymrl/chemgymrl_logo_400x400.png" style="width: 300px; padding: 10px; float: right;">
      
    <p>In early 2019 the lab began a new collaboration sponsored by the <strong>National Research Council â€“ UW Collaboration Centre (NUCC) on AI/Cybersecurity/IoT</strong>. This is a new organization set-up to initial research collaboration between NRC staff researchers and UW PIs. I am one of the first faculty to be a part of this endeavour and to receive funding for my work. With Dr. Isaac Tamblyn (NRC) our project studies how to automate material synthesis and discovery by training a Deep Reinforcement Learning system to plan and carry out chemical synthesis experiments to gather data and find efficient pathways to making new or known materials.</p>

<p>Go take a look at the current framework to carry out your own experiments: <strong><a href="https://chemgymrl.com/">chemgymrl.com</a></strong></p>

  </article>


      
          
            <div class="publications">
              <h2>Our Papers on ChemGymRL</h2> 
            <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Amrl</abbr>
    
  
  </div>

  <div id="bellinger2021canai" class="col-sm-8">
    
      <div class="title">Active Measure Reinforcement Learning for Observation Cost Minimization: A framework for minimizing measurement costs in reinforcement learning</div>
      <div class="author">
        
          
            
              
                
                  Bellinger, Colin,
                
              
            
          
        
          
            
              
                
                  Coles, Rory,
                
              
            
          
        
          
            
              
                <em>Crowley, Mark</em>,
              
            
          
        
          
            
              
                
                  and Tamblyn, Isaac
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Canadian Conference on Artificial Intelligence</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2005.12697" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2021-canai-bellinger-active%20measure%20reinforcement.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Markov Decision Processes (MDP) with explicit measurement cost are a class of en- vironments in which the agent learns to maximize the costed return. Here, we define the costed return as the discounted sum of rewards minus the sum of the explicit cost of measuring the next state. The RL agent can freely explore the relationship between actions and rewards but is charged each time it measures the next state. Thus, an op- timal agent must learn a policy without making a large number of measurements. We propose the active measure RL framework (Amrl) as a solution to this novel class of problem, and contrast it with standard reinforcement learning under full observability and planning under partially observability. We demonstrate that Amrl-Q agents learn to shift from a reliance on costly measurements to exploiting a learned transition model in order to reduce the number of real-world measurements and achieve a higher costed return. Our results demonstrate the superiority of Amrl-Q over standard RL methods, Q-learning and Dyna-Q, and POMCP for planning under a POMDP in environments with explicit measurement costs.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bellinger2020reinforcement" class="col-sm-8">
    
      <div class="title">Reinforcement Learning in a Physics-Inspired Semi-Markov Environment</div>
      <div class="author">
        
          
            
              
                
                  Bellinger, Colin,
                
              
            
          
        
          
            
              
                
                  Coles, Rory,
                
              
            
          
        
          
            
              
                <em>Crowley, Mark</em>,
              
            
          
        
          
            
              
                
                  and Tamblyn, Isaac
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Canadian Conference on Artificial Intelligence</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
            </div>
          
      

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Mark  Crowley.
    Based on [*folio](https://github.com/bogoli/-folio) design. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: February 26, 2022.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
