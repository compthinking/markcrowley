%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Mark Crowley at 2022-01-29 15:24:52 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@unpublished{ghojogh2021arxiv,
	arxiv = {2104.01525},
	author = {Benyamin Ghojogh and Ali Ghodsi and Fakhri Karray and Mark Crowley},
	booktitle = {Preprint},
	date-added = {2022-01-29 15:24:13 -0500},
	date-modified = {2022-01-29 15:24:13 -0500},
	in-cv = {1},
	in-website = {1},
	keywords = {year-in-review-2021, embeddings, representation-learning, manifold-learning},
	publicationstatus = {preprint},
	self = {1},
	status = {-},
	title = {Generative Locally Linear Embedding},
	venue-short = {Arxiv},
	year = {2021},
	abstract = {Locally Linear Embedding (LLE) is a nonlin- ear spectral dimensionality reduction and manifold learning method. It has two main steps which are linear reconstruc- tion and linear embedding of points in the input space and embedding space, respectively. In this work, we propose two novel generative versions of LLE, named Generative LLE (GLLE), whose linear reconstruction steps are stochastic rather than deterministic. GLLE assumes that every data point is caused by its linear reconstruction weights as latent factors. The proposed GLLE algorithms can generate various LLE embeddings stochastically while all the generated embeddings relate to the original LLE embedding. We propose two versions for stochastic linear reconstruction, one using expectation maximization and another with direct sampling from a derived distribution by optimization. The proposed GLLE methods are closely related to and inspired by variational inference, factor analysis, and probabilistic principal component analysis. Our simulations show that the proposed GLLE methods work effectively in unfolding and generating submanifolds of data.},
	annote = {rejected from SMC, very amateur reviews, we'll find somewhere else},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAvLi4vYXNzZXRzL3BkZi8yMDIxLWFyeGl2LWdob2pvZ2gtZ2VuZXJhdGl2ZS5wZGZPEQHGAAAAAAHGAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fMjAyMS1hcnhpdi1naG9qb2doI0ZGRkZGRkZGLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAMAAAogY3UAAAAAAAAAAAAAAAAAA3BkZgAAAgBWLzpVc2VyczptY3Jvd2xleTpEb2N1bWVudHM6bWFya2Nyb3dsZXktY2E6YXNzZXRzOnBkZjoyMDIxLWFyeGl2LWdob2pvZ2gtZ2VuZXJhdGl2ZS5wZGYADgBEACEAMgAwADIAMQAtAGEAcgB4AGkAdgAtAGcAaABvAGoAbwBnAGgALQBnAGUAbgBlAHIAYQB0AGkAdgBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBUVXNlcnMvbWNyb3dsZXkvRG9jdW1lbnRzL21hcmtjcm93bGV5LWNhL2Fzc2V0cy9wZGYvMjAyMS1hcnhpdi1naG9qb2doLWdlbmVyYXRpdmUucGRmABMAAS8AABUAAgAP//8AAAAIAA0AGgAkAFYAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACIA==}}

@article{ghojogh2021factor,
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:2101.00734},
	self = {1},
	status = {-},
	title = {Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey},
	venue-short = {arXiv},
	year = {2021}}

@article{ghojogh2021laplacian,
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:2106.02154},
	self = {1},
	status = {-},
	title = {Laplacian-Based Dimensionality Reduction Including Spectral Clustering, Laplacian Eigenmap, Locality Preserving Projection, Graph Embedding, and Diffusion Map: Tutorial and Survey},
	venue-short = {arXiv},
	year = {2021}}

@article{ghojogh2021reproducing,
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:2106.08443},
	self = {1},
	status = {-},
	title = {Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr$\backslash$" om Method, and Use of Kernels in Machine Learning: Tutorial and Survey},
	venue-short = {arXiv},
	year = {2021}}

@article{ghojogh2020fisher,
	author = {Ghojogh, Benyamin and Sikaroudi, Milad and Shafiei, Sobhan and Tizhoosh, Hamid R and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:2004.04674},
	self = {1},
	status = {-},
	title = {Fisher Discriminant Triplet and Contrastive Losses for Training Siamese Networks},
	venue-short = {arXiv},
	year = {2020}}

@article{ghojogh2020locally,
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:2011.10925},
	self = {1},
	status = {-},
	title = {Locally Linear Embedding and its Variants: Tutorial and Survey},
	venue-short = {arXiv},
	year = {2020}}

@article{ghojogh2020multidimensional,
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:2009.08136},
	self = {1},
	status = {-},
	title = {Multidimensional scaling, Sammon mapping, and Isomap: Tutorial and survey},
	venue-short = {arXiv},
	year = {2020}}

@article{ghojogh2020roweisposes,
	author = {Ghojogh, Benyamin and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:2006.15736},
	self = {1},
	status = {-},
	title = {Roweisposes, Including Eigenposes, Supervised Eigenposes, and Fisherposes, for 3D Action Recognition},
	venue-short = {arXiv},
	year = {2020}}

@article{ghojogh2020sampling,
	author = {Ghojogh, Benyamin and Nekoei, Hadi and Ghojogh, Aydin and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:2011.00901},
	self = {1},
	status = {-},
	title = {Sampling algorithms, from survey sampling to Monte Carlo methods: Tutorial and literature review},
	venue-short = {arXiv},
	year = {2020}}

@article{ghojogh2020stochastic,
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:2009.10301},
	self = {1},
	status = {-},
	title = {Stochastic neighbor embedding with Gaussian and Student-t distributions: Tutorial and survey},
	venue-short = {arXiv},
	year = {2020}}

@unpublished{ghojogh2019feature,
	author = {Ghojogh, Benyamin and Samad, Maria N and Mashhadi, Sayema Asif and Kapoor, Tania and Ali, Wahab and Karray, Fakhri and Crowley, Mark},
	citations = {37},
	citations-non-self = {?},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	in-cv = {1},
	journal = {arXiv preprint arXiv:1905.02845},
	self = {1},
	status = {-},
	title = {Feature selection and feature extraction in pattern analysis: A literature review},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019theory,
	author = {Ghojogh, Benyamin and Crowley, Mark},
	citations = {40},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:31 -0400},
	in-cv = {1},
	journal = {arXiv preprint arXiv:1905.12787},
	self = {1},
	self-citations = {?},
	status = {-},
	title = {The theory behind overfitting, cross validation, regularization, bagging, and boosting: tutorial},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019linear,
	author = {Ghojogh, Benyamin and Crowley, Mark},
	citations = {24},
	citations-non-self = {?},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	in-cv = {1},
	journal = {arXiv preprint arXiv:1906.02590},
	self = {1},
	status = {-},
	title = {Linear and Quadratic Discriminant Analysis: Tutorial},
	venue-short = {arXiv},
	year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBPcGRmcy8yMDE5LWFyeGl2LWdob2pvZ2gtbGluZWFyIGFuZCBxdWFkcmF0aWMgZGlzY3JpbWluYW50IGFuYWx5c2lzIHR1dG9yaWFsLnBkZk8RAnwAAAAAAnwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x8yMDE5LWFyeGl2LWdob2pvZ2gjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEAAwAACiBjdQAAAAAAAAAAAAAAAAAEcGRmcwACAIcvOlVzZXJzOm1jcm93bGV5OkRvY3VtZW50czptYXJrY3Jvd2xleS1jYTpfYmlibGlvZ3JhcGh5OnBkZnM6MjAxOS1hcnhpdi1naG9qb2doLWxpbmVhciBhbmQgcXVhZHJhdGljIGRpc2NyaW1pbmFudCBhbmFseXNpcyB0dXRvcmlhbC5wZGYAAA4AlgBKADIAMAAxADkALQBhAHIAeABpAHYALQBnAGgAbwBqAG8AZwBoAC0AbABpAG4AZQBhAHIAIABhAG4AZAAgAHEAdQBhAGQAcgBhAHQAaQBjACAAZABpAHMAYwByAGkAbQBpAG4AYQBuAHQAIABhAG4AYQBsAHkAcwBpAHMAIAB0AHUAdABvAHIAaQBhAGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAIVVc2Vycy9tY3Jvd2xleS9Eb2N1bWVudHMvbWFya2Nyb3dsZXktY2EvX2JpYmxpb2dyYXBoeS9wZGZzLzIwMTktYXJ4aXYtZ2hvam9naC1saW5lYXIgYW5kIHF1YWRyYXRpYyBkaXNjcmltaW5hbnQgYW5hbHlzaXMgdHV0b3JpYWwucGRmAAATAAEvAAAVAAIAD///AAAACAANABoAJAB2AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAvY=},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1906.02590.pdf}}

@unpublished{ghojogh2019unsupervised,
	author = {Ghojogh, Benyamin and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:1906.03148},
	self = {1},
	status = {-},
	title = {Unsupervised and supervised principal component analysis: Tutorial},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019fisher,
	author = {Ghojogh, Benyamin and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:1906.09436},
	self = {1},
	status = {-},
	title = {Fisher and kernel fisher discriminant analysis: Tutorial},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019quantized,
	author = {Ghojogh, Benyamin and Pasand, Ali Saheb and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:1909.03037},
	self = {1},
	status = {-},
	title = {Quantized Fisher Discriminant Analysis},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019distributed,
	author = {Ghojogh, Benyamin and Salehkaleybar, Saber},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	journal = {arXiv preprint arXiv:1910.09882},
	self = {1},
	status = {-},
	title = {Distributed Voting in Beep Model},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019hidden,
	author = {Ghojogh, Benyamin and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	publisher = {engrXiv},
	self = {1},
	status = {-},
	title = {Hidden Markov Model: Tutorial},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019eigenvalue,
	archiveprefix = {arXiv},
	arxiv = {1903.11240},
	arxivid = {1903.11240},
	author = {Ghojogh, Benyamin and Karray, Fakhri and Crowley, Mark},
	booktitle = {ArXiv Preprint arXiv:1903.11240},
	citations = {35},
	citations-non-self = {?},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	eprint = {1903.11240},
	in-cv = {1},
	keywords = {Manifold Learning, Data Reduction, Numerosity Reduction},
	self = {1},
	self-citations = {?},
	status = {2},
	title = {{Eigenvalue and Generalized Eigenvalue Problems: Tutorial}},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019fitting,
	archiveprefix = {arXiv},
	arxivid = {1901.06708},
	author = {Ghojogh, Benyamin and Ghojogh, Aydin and Crowley, Mark and Karray, Fakhri},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	eprint = {1901.06708},
	journal = {ArXiv preprint. arXiv:1901.06708},
	keywords = {Parameter Estimation},
	self = {1},
	status = {2},
	title = {{Fitting A Mixture Distribution to Data: Tutorial}},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019addressing,
	archiveprefix = {arXiv},
	arxivid = {1903.06671},
	author = {Ghojogh, Benyamin and Crowley, Mark and Karray, Fakhri},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	eprint = {1903.06671},
	journal = {ArXiv Preprint. ArXiv: 1903.06671},
	keywords = {Prediction, Computational Sustainability},
	self = {1},
	status = {2},
	title = {{Addressing the Mystery of Population Decline of the Rose-Crested Blue Pipit in a Nature Preserve using Data Visualization}},
	venue-short = {arXiv},
	year = {2019}}

@unpublished{ghojogh2019rda,
	archiveprefix = {arXiv},
	arxiv = {1910.05437},
	arxivid = {1910.05437},
	author = {Ghojogh, Benyamin and Karray, Fakhri and Crowley, Mark},
	date-added = {2021-07-07 15:42:16 -0400},
	date-modified = {2021-07-07 15:42:16 -0400},
	eprint = {1910.05437},
	file = {:Users/mcrowley/Dropbox/MendeleyDesktop/Ghojogh, Karray, Crowley - 2019 - Roweis Discriminant Analysis A Generalized Subspace Learning Method.pdf:pdf},
	month = {oct},
	publicationstatus = {preprint},
	self = {1},
	status = {-},
	title = {{Roweis Discriminant Analysis: A Generalized Subspace Learning Method}},
	url = {http://arxiv.org/abs/1910.05437},
	venue-short = {arXiv},
	year = {2019},
	abstract = {We present a new method which generalizes subspace learning based on eigenvalue and generalized eigenvalue problems. This method, Roweis Discriminant Analysis (RDA), is named after Sam Roweis to whom the field of subspace learning owes significantly. RDA is a family of infinite number of algorithms where Principal Component Analysis (PCA), Supervised PCA (SPCA), and Fisher Discriminant Analysis (FDA) are special cases. One of the extreme special cases, which we name Double Supervised Discriminant Analysis (DSDA), uses the labels twice; it is novel and has not appeared elsewhere. We propose a dual for RDA for some special cases. We also propose kernel RDA, generalizing kernel PCA, kernel SPCA, and kernel FDA, using both dual RDA and representation theory. Our theoretical analysis explains previously known facts such as why SPCA can use regression but FDA cannot, why PCA and SPCA have duals but FDA does not, why kernel PCA and kernel SPCA use kernel trick but kernel FDA does not, and why PCA is the best linear method for reconstruction. Roweisfaces and kernel Roweisfaces are also proposed generalizing eigenfaces, Fisherfaces, supervised eigenfaces, and their kernel variants. We also report experiments showing the effectiveness of RDA and kernel RDA on some benchmark datasets.},
	Bdsk-Url-1 = {http://arxiv.org/abs/1910.05437}}
