<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mark  Crowley | Multi-Agent Reinforcement Learning</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/marl/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Mark</span>   Crowley
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/bio/">
                bio
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/news/">
                news
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
      
          
            <h1 class="post-title">Multi-Agent Reinforcement Learning</h1>
        
      
    <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:10px;">MARL is the problem of learning how to make decisions from experience in the presence of multiple other decision making agents.</p>

      
      
      
      
      
      
        <p></p>
  </header>

  <article>
      
    <p>Standard <a href="/keywords/Reinforcement-Learning">Reinforcement Learning</a> studies how to build computational agents that can learn how to make decisions from interaction from their environment alone, even without a prior understanding of how that that environment works. This field is closely connected with human and animal learning and uses the idea of <em>rewards</em> obtained implicitely from the environment or explicitely from a trainer.</p>

<p>The field of <strong>Multi-Agent Reinforcement Learning</strong> has been growing steadily interest and complexity in recent years. This is RL in the more complex situation where there are <em>other agents</em> in the environment to interact with who also impact the reward obtained by our learning agent. Now these other agents could be teammates, oponents or neutral strangers and the learning agent might interact directly or indirectly with them. <a href="/keywords/gametheory/">Game Theory</a> is the study of a particular subset of this problem where the domain is generally well understood and while there may be <em>hidden information</em> which agents may not have access to, the agents do not need to <em>learn</em> about the environment itself in order to take actions.</p>


  </article>


      
          
            <div class="publications">
              <h2>Our Papers on Multi-Agent Reinforcement Learning</h2> 
            <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganapathi-subramanian2022ijcai" class="col-sm-8">
    
      <div class="title">Learning from Multiple Independent Advisors in Multi-agent Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                
                  Taylor, Mathew,
                
              
            
          
        
          
            
              
                
                  Larson, Kate,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Joint Conference on Artificial Intelligence (IJCAI)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Multi-agent reinforcement learning typically suffers from the problem of sample efficiency, where learning suitable policies involve the use of many data samples. Learning from external demonstrators is a possible solution for this problem, however almost all previous approaches assume the presence of a single demonstrator that can be followed. Leveraging multiple knowledge sources (broadly ‘advisors’) that have expertise in distinct aspects of the environment could substantially speed up learning in complex environments. In this paper, we study the problem of simultaneously learning from multiple independent advisors in multi-agent reinforcement learning. Particularly, we adapt the two-level Q-learning architecture that was previously introduced for the single-agent setting, to multi-agent environments. We provide principled algorithms that systematically incorporate a set of advisors by evaluating the value of the advisors and using the advisors for action selection. We provide theoretical convergence and sample complexity guarantees. Experimentally, we validate our approach in three different test-beds and show that our algorithms give a stronger performance compared to baselines, can effectively integrate the combined expertise of different advisors, and ignore the bad advisors.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Mean Field MARL</abbr>
    
  
  </div>

  <div id="ganapathisubramanian2022aaai" class="col-sm-8">
    
      <div class="title">Decentralized Mean Field Games</div>
      <div class="author">
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                
                  Taylor, Mathew,
                
              
            
          
        
          
            
              
                <em>Crowley, Mark</em>,
              
            
          
        
          
            
              
                
                  and Poupart, Pascal
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI-2022)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Multiagent reinforcement learning algorithms have not been widely adopted in large scale environments with many agents as they often scale poorly with the number of agents. Using mean field theory to aggregate agents has been proposed as a solution to this problem. However, almost all previous methods in this area make a strong assumption of a centralized system where all the agents in the environment learn the same policy and are effectively indistinguishable from each other. In this paper, we relax this assumption about indistinguishable agents and propose a new mean field system known as Decentralized Mean Field Games, where each agent can be quite different from others. All agents learn independent policies in a decentralized fashion, based on their local observations. We define a theoretical solution concept for this system and provide a fixed point guarantee for a Q-learning based algorithm in this system. A practical consequence of our approach is that we can address a ‘chicken-and-egg’ problem in empirical mean field reinforcement learning algorithms. Further, we provide Q-learning and actor-critic algorithms that use the decentralized mean field learning approach and give stronger performances compared to common baselines in this area. In our setting, agents do not need to be clones of each other and learn in a fully decentralized fashion. Hence, for the first time, we show the application of mean field learning methods in fully competitive environments, large-scale continuous action space environments, and other environments with heterogeneous agents. Importantly, we also apply the mean field method in a ride-sharing problem using a real-world dataset. We propose a decentralized solution to this problem, which is more practical than existing centralized training methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MARLEmpircal</abbr>
    
  
  </div>

  <div id="lee2021frontai" class="col-sm-8">
    
      <div class="title">Investigation of Independent Reinforcement Learning Algorithms in Multi-Agent Environments</div>
      <div class="author">
        
          
            
              
                
                  Lee, Ken Ming,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Frontiers in Artificial Intelligence (Machine Learning and Artificial Intelligence)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MARLEmpircal</abbr>
    
  
  </div>

  <div id="lee2021neuripsdeeprl" class="col-sm-8">
    
      <div class="title">Investigation of Independent Reinforcement Learning Algorithms in Multi-Agent Environments</div>
      <div class="author">
        
          
            
              
                
                  Lee, Ken Ming,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In NeurIPS 2021 Deep Reinforcement Learning Workshop</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/http://arxiv.org/abs/2111.01100" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Independent reinforcement learning algorithms have no theoretical guarantees for finding the best policy in multi-agent settings. However, in practice, prior works have reported good performance with independent algorithms in some domains and bad performance in others. Moreover, a comprehensive study of the strengths and weaknesses of independent algorithms is lacking in the literature. In this paper, we carry out an empirical comparison of the performance of independent algorithms on four PettingZoo environments that span the three main categories of multi-agent environments, i.e., cooperative, competitive, and mixed. We show that in fully-observable environments, independent algorithms can perform on par with multi-agent algorithms in cooperative and competitive settings. For the mixed environments, we show that agents trained via independent algorithms learn to perform well individually, but fail to learn to cooperate with allies and compete with enemies. We also show that adding recurrence improves the learning of independent algorithms in cooperative partially observable environments.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganapathisubramanian2021aamas" class="col-sm-8">
    
      <div class="title">Partially Observable Mean Field Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                
                  Taylor, Matthew,
                
              
            
          
        
          
            
              
                <em>Crowley, Mark</em>,
              
            
          
        
          
            
              
                
                  and Poupart, Pascal
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2012.15791" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2021-aamas-ganapathi%20subramanian-partially.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Traditional multi-agent reinforcement learning algorithms are not scalable to environments with more than a few agents, since these algorithms are exponential in the number of agents. Recent research has introduced successful methods to scale multi-agent reinforcement learning algorithms to many agent scenarios using mean field theory. Previous work in this field assumes that an agent has access to exact cumulative metrics regarding the mean field behaviour of the system, which it can then use to take its actions. In this paper, we relax this assumption and maintain a distribution to model the uncertainty regarding the mean field of the system. We consider two different settings for this problem. In the first setting, only agents in a fixed neighbourhood are visible, while in the second setting, the visibility of agents is determined at random based on distances. For each of these settings, we introduce a Q-learning based algorithm that can learn effectively. We prove that this Q-learning estimate stays very close to the Nash Q-value (under a common set of assumptions) for the first setting. We also empirically show our algorithms outperform multiple baselines in three different games in the MAgents framework, which supports large environments with many agents learning simultaneously to achieve possibly distinct goals.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bhalla2020deep" class="col-sm-8">
    
      <div class="title">Deep Multi Agent Reinforcement Learning for Autonomous Driving</div>
      <div class="author">
        
          
            
              
                
                  Bhalla, Sushrut,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Canadian Conference on Artificial Intelligence</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/2020-canai-bhalla-deep.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-47358-7_7" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Bhalla2019rldm" class="col-sm-8">
    
      <div class="title">Learning Multi-Agent Communication with Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Bhalla, Sushrut,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Conference on Reinforcement Learning and Decision Making (RLDM-19)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Bhalla2019aamas" class="col-sm-8">
    
      <div class="title">Training Cooperative Agents for Multi-Agent Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Bhalla, Sushrut,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>
            </div>
          
      

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Mark  Crowley.
    Based on [*folio](https://github.com/bogoli/-folio) design. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: February 26, 2022.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
