<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Mark  Crowley | Reinforcement Learning</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/reinforcement-learning/">

<!-- Theming-->


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Mark</span>   Crowley
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/bio/">
                bio
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/news/">
                news
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
      
          
            <h1 class="post-title">Reinforcement Learning</h1>
        
      
    <p class="post-description" style="border-bottom-style:dashed; border-bottom-color:lightgrey; border-bottom-width:1px; margin:1px; margin-bottom:10px;">RL is the study of learning decision making policies from experience with computers.</p>

      
      
      
      
      
      
        <p></p>
  </header>

  <article>
      
      <img src="/assets/img/teaching/ece493-logo.png" style="width: 300px; padding: 10px; float: right;">
      
    <p>One of my core research areas is into understanding the computational mechanisms that can enable learning to perform complex tasks primarily from experience and feedback. This topic, called <strong><em>Reinforcement Learning</em></strong>,  has a complex history tying fields as diverse as neuroscience, behavioural and development psychology, economics and computer science. I approach it as a computational researcher aiming to build Artificial Intelligence agents that learn to way Humans do, not by any correspondence of their “brain” and it “neural” structure by the <em>algorithms they both use to learn to act in a complex, mysterious world.</em></p>

<h2 id="learning-resources">Learning Resources</h2>
<h3 id="courses-and-texts">Courses and Texts</h3>
<ul>
  <li>The Textbook - http://incompleteideas.net/book/the-book-2nd.html</li>
  <li><a href="https://www.coursera.org/specializations/reinforcement-learning?utm_source=gg&amp;utm_medium=sem&amp;utm_content=04-ReinforcementLearning-UA-CA&amp;campaignid=6770937312&amp;adgroupid=85996872692&amp;device=c&amp;keyword=reinforcement%20learning%20course&amp;matchtype=b&amp;network=g&amp;devicemodel=&amp;adpostion=&amp;creativeid=391979104237&amp;hide_mobile_promo&amp;gclid=Cj0KCQjwm9D0BRCMARIsAIfvfIYKjEq7S-DqrGVUNrH6GIcvwMRPX4tz_1LgKbgnt7nm2c-cvtAHy3YaAu9xEALw_wcB">Martha White’s RL Fundamentals Course</a></li>
  <li><a href="http://rail.eecs.berkeley.edu/deeprlcourse/">Sergey Levine’s Deep RL Course</a></li>
</ul>

<h3 id="seminal-deep-rl-papers">Seminal Deep RL Papers</h3>
<ul>
  <li>https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf</li>
</ul>

<h3 id="tools">Tools</h3>
<ul>
  <li>https://gym.openai.com/</li>
</ul>


  </article>


      
          
            <div class="publications">
              <h2>Our Papers on Reinforcement Learning</h2> 
            <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganapathi-subramanian2022ijcai" class="col-sm-8">
    
      <div class="title">Learning from Multiple Independent Advisors in Multi-agent Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                
                  Taylor, Mathew,
                
              
            
          
        
          
            
              
                
                  Larson, Kate,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Joint Conference on Artificial Intelligence (IJCAI)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Multi-agent reinforcement learning typically suffers from the problem of sample efficiency, where learning suitable policies involve the use of many data samples. Learning from external demonstrators is a possible solution for this problem, however almost all previous approaches assume the presence of a single demonstrator that can be followed. Leveraging multiple knowledge sources (broadly ‘advisors’) that have expertise in distinct aspects of the environment could substantially speed up learning in complex environments. In this paper, we study the problem of simultaneously learning from multiple independent advisors in multi-agent reinforcement learning. Particularly, we adapt the two-level Q-learning architecture that was previously introduced for the single-agent setting, to multi-agent environments. We provide principled algorithms that systematically incorporate a set of advisors by evaluating the value of the advisors and using the advisors for action selection. We provide theoretical convergence and sample complexity guarantees. Experimentally, we validate our approach in three different test-beds and show that our algorithms give a stronger performance compared to baselines, can effectively integrate the combined expertise of different advisors, and ignore the bad advisors.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SubWorld</abbr>
    
  
  </div>

  <div id="beeler2022ieeeintsys" class="col-sm-8">
    
      <div class="title">Dynamic programming with partial information to overcome navigational uncertainty in a nautical environment</div>
      <div class="author">
        
          
            
              
                
                  Beeler, Chris,
                
              
            
          
        
          
            
              
                
                  Li, Xinkai,
                
              
            
          
        
          
            
              
                <em>Crowley, Mark</em>,
              
            
          
        
          
            
              
                
                  Fraser, Maia,
                
              
            
          
        
          
            
              
                
                  and Tamblyn, Isaac
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Intelligent Systems</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Using a toy nautical navigation environment, we show that dynamic
programming can be used when only partial information about a partially
observed Markov decision process (POMDP) is known. By incorporating
uncertainty into our model, we show that navigation policies can be
constructed that maintain safety. Adding controlled sensing methods, we
show that these policies can also lower measurement costs at the same
time.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bellinger2022ai2ase" class="col-sm-8">
    
      <div class="title">Scientific Discovery and the Cost of Measurement – Balancing Information and Cost in Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Bellinger, Colin,
                
              
            
          
        
          
            
              
                
                  Drozdyuk, Andriy,
                
              
            
          
        
          
            
              
                <em>Crowley, Mark</em>,
              
            
          
        
          
            
              
                
                  and Tamblyn, Isaac
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 1st Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2112.07535" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The use of reinforcement learning (RL) in scientific applications, such as materials design and automated chemistry, is increasing. A major challenge, however, lies in fact that measuring the state of the system is often costly and time consuming in scientific applications, whereas policy learning with RL requires a measurement after each time step. In this work, we make the measurement costs explicit in the form of a costed reward and propose a framework that enables off-the-shelf deep RL algorithms to learn a policy for both selecting actions and determining whether or not to measure the current state of the system at each time step. In this way, the agents learn to balance the need for information with the cost of information. Our results show that when trained under this regime, the Dueling DQN and PPO agents can learn optimal action policies whilst making up to 50% fewer state measurements, and recurrent neural networks can produce a greater than 50% reduction in measurements. We postulate the these reduction can help to lower the barrier to applying RL to real-world scientific applications.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Mean Field MARL</abbr>
    
  
  </div>

  <div id="ganapathisubramanian2022aaai" class="col-sm-8">
    
      <div class="title">Decentralized Mean Field Games</div>
      <div class="author">
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                
                  Taylor, Mathew,
                
              
            
          
        
          
            
              
                <em>Crowley, Mark</em>,
              
            
          
        
          
            
              
                
                  and Poupart, Pascal
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI-2022)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Multiagent reinforcement learning algorithms have not been widely adopted in large scale environments with many agents as they often scale poorly with the number of agents. Using mean field theory to aggregate agents has been proposed as a solution to this problem. However, almost all previous methods in this area make a strong assumption of a centralized system where all the agents in the environment learn the same policy and are effectively indistinguishable from each other. In this paper, we relax this assumption about indistinguishable agents and propose a new mean field system known as Decentralized Mean Field Games, where each agent can be quite different from others. All agents learn independent policies in a decentralized fashion, based on their local observations. We define a theoretical solution concept for this system and provide a fixed point guarantee for a Q-learning based algorithm in this system. A practical consequence of our approach is that we can address a ‘chicken-and-egg’ problem in empirical mean field reinforcement learning algorithms. Further, we provide Q-learning and actor-critic algorithms that use the decentralized mean field learning approach and give stronger performances compared to common baselines in this area. In our setting, agents do not need to be clones of each other and learn in a fully decentralized fashion. Hence, for the first time, we show the application of mean field learning methods in fully competitive environments, large-scale continuous action space environments, and other environments with heterogeneous agents. Importantly, we also apply the mean field method in a ride-sharing problem using a real-world dataset. We propose a decentralized solution to this problem, which is more practical than existing centralized training methods.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MARLEmpircal</abbr>
    
  
  </div>

  <div id="lee2021frontai" class="col-sm-8">
    
      <div class="title">Investigation of Independent Reinforcement Learning Algorithms in Multi-Agent Environments</div>
      <div class="author">
        
          
            
              
                
                  Lee, Ken Ming,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Frontiers in Artificial Intelligence (Machine Learning and Artificial Intelligence)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">MARLEmpircal</abbr>
    
  
  </div>

  <div id="lee2021neuripsdeeprl" class="col-sm-8">
    
      <div class="title">Investigation of Independent Reinforcement Learning Algorithms in Multi-Agent Environments</div>
      <div class="author">
        
          
            
              
                
                  Lee, Ken Ming,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In NeurIPS 2021 Deep Reinforcement Learning Workshop</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/http://arxiv.org/abs/2111.01100" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Independent reinforcement learning algorithms have no theoretical guarantees for finding the best policy in multi-agent settings. However, in practice, prior works have reported good performance with independent algorithms in some domains and bad performance in others. Moreover, a comprehensive study of the strengths and weaknesses of independent algorithms is lacking in the literature. In this paper, we carry out an empirical comparison of the performance of independent algorithms on four PettingZoo environments that span the three main categories of multi-agent environments, i.e., cooperative, competitive, and mixed. We show that in fully-observable environments, independent algorithms can perform on par with multi-agent algorithms in cooperative and competitive settings. For the mixed environments, we show that agents trained via independent algorithms learn to perform well individually, but fail to learn to cooperate with allies and compete with enemies. We also show that adding recurrence improves the learning of independent algorithms in cooperative partially observable environments.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Multi-Advisor-QL</abbr>
    
  
  </div>

  <div id="ganapathisubramanian2021jair" class="col-sm-8">
    
      <div class="title">Multi-Agent Advisor Q-Learning</div>
      <div class="author">
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                
                  Larson, Kate,
                
              
            
          
        
          
            
              
                
                  Taylor, Mathew,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Artificial Intelligence Research (JAIR)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="crowley2020coviddata" class="col-sm-8">
    
      <div class="title">Prediction and Causality: How Can Machine Learning be Used for COVID-19?</div>
      <div class="author">
        
          
            
              <em>Crowley, Mark</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>In "What Needs to be done in order to Curb the Spread of Covid-19: Exposure Notification, Legal Considerations, and Statistical Modeling", a Conference on Data and Privacy during a Global Pandemic</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/pdfs/2021-coviddata-crowley-prediction.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
      
      <a href="/assets/pdf/pdfs/2021-coviddata-crowley-prediction.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
      <a href="https://uwaterloo.ca/master-of-public-service/events/data-and-privacy-during-global-pandemic-conference" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="subramanian2018neurips-ai4sg" class="col-sm-8">
    
      <div class="title">A Complementary Approach to Improve WildFire Prediction Systems.</div>
      <div class="author">
        
          
            
              
                
                  Subramanian, Sriram Ganapathi,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Neural Information Processing Systems (AI for social good workshop)</em>
      
      
        2018
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/2018-neurips-ai-subramanian-a%20complementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://aiforsocialgood.github.io/2018/acceptedpapers.htm" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="ganapathisubramanian2021aamas" class="col-sm-8">
    
      <div class="title">Partially Observable Mean Field Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                
                  Taylor, Matthew,
                
              
            
          
        
          
            
              
                <em>Crowley, Mark</em>,
              
            
          
        
          
            
              
                
                  and Poupart, Pascal
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2012.15791" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2021-aamas-ganapathi%20subramanian-partially.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Traditional multi-agent reinforcement learning algorithms are not scalable to environments with more than a few agents, since these algorithms are exponential in the number of agents. Recent research has introduced successful methods to scale multi-agent reinforcement learning algorithms to many agent scenarios using mean field theory. Previous work in this field assumes that an agent has access to exact cumulative metrics regarding the mean field behaviour of the system, which it can then use to take its actions. In this paper, we relax this assumption and maintain a distribution to model the uncertainty regarding the mean field of the system. We consider two different settings for this problem. In the first setting, only agents in a fixed neighbourhood are visible, while in the second setting, the visibility of agents is determined at random based on distances. For each of these settings, we introduce a Q-learning based algorithm that can learn effectively. We prove that this Q-learning estimate stays very close to the Nash Q-value (under a common set of assumptions) for the first setting. We also empirically show our algorithms outperform multiple baselines in three different games in the MAgents framework, which supports large environments with many agents learning simultaneously to achieve possibly distinct goals.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Amrl</abbr>
    
  
  </div>

  <div id="bellinger2021canai" class="col-sm-8">
    
      <div class="title">Active Measure Reinforcement Learning for Observation Cost Minimization: A framework for minimizing measurement costs in reinforcement learning</div>
      <div class="author">
        
          
            
              
                
                  Bellinger, Colin,
                
              
            
          
        
          
            
              
                
                  Coles, Rory,
                
              
            
          
        
          
            
              
                <em>Crowley, Mark</em>,
              
            
          
        
          
            
              
                
                  and Tamblyn, Isaac
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Canadian Conference on Artificial Intelligence</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2005.12697" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
      
      <a href="/assets/pdf/2021-canai-bellinger-active%20measure%20reinforcement.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Markov Decision Processes (MDP) with explicit measurement cost are a class of en- vironments in which the agent learns to maximize the costed return. Here, we define the costed return as the discounted sum of rewards minus the sum of the explicit cost of measuring the next state. The RL agent can freely explore the relationship between actions and rewards but is charged each time it measures the next state. Thus, an op- timal agent must learn a policy without making a large number of measurements. We propose the active measure RL framework (Amrl) as a solution to this novel class of problem, and contrast it with standard reinforcement learning under full observability and planning under partially observability. We demonstrate that Amrl-Q agents learn to shift from a reliance on costly measurements to exploiting a learned transition model in order to reduce the number of real-world measurements and achieve a higher costed return. Our results demonstrate the superiority of Amrl-Q over standard RL methods, Q-learning and Dyna-Q, and POMCP for planning under a POMDP in environments with explicit measurement costs.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="bhalla2020deep" class="col-sm-8">
    
      <div class="title">Deep Multi Agent Reinforcement Learning for Autonomous Driving</div>
      <div class="author">
        
          
            
              
                
                  Bhalla, Sushrut,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Canadian Conference on Artificial Intelligence</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="/assets/pdf/2020-canai-bhalla-deep.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-47358-7_7" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Bhalla2019rldm" class="col-sm-8">
    
      <div class="title">Learning Multi-Agent Communication with Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Bhalla, Sushrut,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Conference on Reinforcement Learning and Decision Making (RLDM-19)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Bhalla2019aamas" class="col-sm-8">
    
      <div class="title">Training Cooperative Agents for Multi-Agent Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Bhalla, Sushrut,
                
              
            
          
        
          
            
              
                
                  Ganapathi Subramanian, Sriram,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="subramanian2017rldm" class="col-sm-8">
    
      <div class="title">Learning Forest Wildfire Dynamics from Satellite Images Using Reinforcement Learning</div>
      <div class="author">
        
          
            
              
                
                  Subramanian, Sriram Ganapathi,
                
              
            
          
        
          
            
              
                and <em>Crowley, Mark</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Conference on Reinforcement Learning and Decision Making</em>
      
      
        2017
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Crowley2013" class="col-sm-8">
    
      <div class="title">Policy Gradient Optimization Using Equilibrium Policies for Spatial Planning Domains</div>
      <div class="author">
        
          
            
              <em>Crowley, Mark</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 13th INFORMS Computing Society Conference</em>
      
      
        2013
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">thesis</abbr>
    
  
  </div>

  <div id="Crowley2011thesis" class="col-sm-8">
    
      <div class="title">Equilibrium Policy Gradients for Spatiotemporal Planning</div>
      <div class="author">
        
          
            
              <em>Crowley, Mark</em>
            
          
        
      </div>

      <div class="periodical">
      
      
        2011
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="http://hdl.handle.net/2429/38971" class="btn btn-sm z-depth-0" role="button" target="_blank">URL</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In spatiotemporal planning, agents choose actions at multiple locations in space over some planning horizon to maximize their utility and satisfy various constraints. In forestry planning, for example, the problem is to choose actions for thousands of locations in the forest each year. The actions at each location could include harvesting trees, treating trees against disease and pests, or doing nothing. A utility model could place value on sale of forest products, ecosystem sustainability or employment levels, and could incorporate legal and logistical constraints such as avoiding large contiguous areas of clearcutting and managing road access. Planning requires a model of the dynamics. Existing simulators developed by forestry researchers can provide detailed models of the dynamics of a forest over time, but these simulators are often not designed for use in automated planning. This thesis presents spatiotemoral planning in terms of factored Markov decision processes. A policy gradient planning algorithm optimizes a stochastic spatial policy using existing simulators for dynamics. When a planning problem includes spatial interaction between locations, deciding on an action to carry out at one location requires considering the actions performed at other locations. This spatial interdependence is common in forestry and other environmental planning problems and makes policy representation and planning challenging. We define a spatial policy in terms of local policies defined as distributions over actions at one location conditioned upon actions at other locations. A policy gradient planning algorithm using this spatial policy is presented which uses Markov Chain Monte Carlo simulation to sample the landscape policy, estimate its gradient and use this gradient to guide policy improvement. Evaluation is carried out on a forestry planning problem with 1880 locations using a variety of value models and constraints. The distribution over joint actions at all locations can be seen as the equilibrium of a cyclic causal model. This equilibrium semantics is compared to Structural Equation Models. We also define an algorithm for approximating the equilibrium distribution for cyclic causal networks which exploits graphical structure and analyse when the algorithm is exact.</p>
    </div>
    
  </div>
</div>
</li></ol>
            </div>
          
      

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Mark  Crowley.
    Based on [*folio](https://github.com/bogoli/-folio) design. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: February 26, 2022.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
